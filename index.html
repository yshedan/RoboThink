<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RoboThink: Training-Free VLM Planning for Long-Horizon Robotic Manipulation via Spatial-Aware Chain-of-Thought and Self-Correction</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RoboThink: Training-Free VLM Planning for Long-Horizon Robotic Manipulation via Spatial-Aware Chain-of-Thought and Self-Correction</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://2-mo.github.io/" target="_blank">Shedan Yang</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Jiaxu Leng</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Mengjingcheng Mo</a>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Yunjin Qu</a>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Ji Gan</a>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Haosheng Chen</a>,</span>
              <span class="author-block">
                <a href="https://github.com/Hayneyday" target="_blank">Xinbo Gao</a>,</span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Chongqing University of Posts and Techcommunications<be>...</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2505.21962" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/yshedan" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.21962" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="80%">
        <!-- Your video here -->
        <source src="static/videos/RoboThink_Demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       The execution process of five daily manipulation tasks in the real-world environment.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic manipulation has achieved promising results in simple tasks, yet it remains challenged by long-horizon and complex task settings. Recent efforts have introduced large language models (LLMs) to decompose tasks, partially mitigating these difficulties. However, two key limitations persist: (1) in complex environments, object diversity, occlusions, and environmental disturbances hinder reliable perception and planning; and (2) sequential execution of multi-step tasks is vulnerable to cascading failures from intermediate errors, compromising overall task completion. To address these challenges, we draw inspiration from human cognition, enabling robots to understand inter-object relationships for obstacle-aware planning and dynamically recover from execution failures to enhance robustness.
Motivated by these insights, we propose \textbf{RoboThink}, a vision-language model (VLM)-driven framework for spatial reasoning and error correction in robotic manipulation. RoboThink incorporates spatial information into task planning and continuously reflects on and rectifies execution errors. Specifically, we introduce a spatial-aware chain-of-thought module to guide VLMs in multimodal spatial reasoning, facilitating obstacle-aware planning and trajectory generation. Furthermore, a subtask correction mechanism is devised to exploit model feedback and validation for adaptive error rectification during task execution.
Extensive real-world experiments across six manipulation tasks demonstrate that RoboThink achieves an average success rate of 73.33\%, outperforming the baseline by 36.66\%. On three long-horizon complex tasks of organize the items, wipe the table and clean the table, RoboThink attains a 73.33\% success rate, marking a 46.66\% relative improvement. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Section 1: Overview of the RoboThink -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered">Overview of the RoboThink</h2>
        <div class="content has-text-justified">
        </div>
        <div class="columns is-centered mt-4 mb-4">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/carouse1.png" alt="Overview of the RoboThink">
              <figcaption class="has-text-centered">Overview of RoboThink framework. The RoboThink takes task instructions and RGB images as input, leverages vision-language models (VLM) for spatial reasoning to achieve task planning, trajectory generation, and execution, and ensures accuracy and reliability through a subtask correction module. This method operates without additional training.</figcaption>
            </figure>
          </div>
        </div>

        <div class="columns is-centered mt-4 mb-4">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/carouse2.png" alt="Subtask Correction Module">
              <figcaption class="has-text-centered">Framework diagram of Subtask Correction Module. The subtask correction module consists of two steps: subtask verification and reflection&correction.</figcaption>
            </figure>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>


  
<!-- Section 2: Trajectory planning results visualization -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered">Trajectory planning results visualization</h2>
        <div class="content has-text-justified">
          <p>We demonstrate the trajectory planning results, confirming the effectiveness of the spatial reasoning CoT module. This module identifies obstacles through subtask instructions, generates safe obstacle-avoidance trajectories, and enhances the safety of robotic arm movements.</p>
        </div>
        <div class="columns is-centered mt-4 mb-4">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/carouse3.png" alt="Trajectory planning results visualization" style="width: 80%; height: auto;>
              <figcaption class="has-text-centered">Trajectory planning results visualization. Where, columns (a) display the observation images, columns (b) show the front-view
trajectory diagrams, and columns (c) illustrate the left-view trajectory diagrams, column (d) displays the observation image after completing the trajectory
(b). </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  
<!-- Section 3: subtask correction process visualization -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered"> Subtask correction process visualization</h2>
        <div class="content has-text-justified">
          <p>We show the subtask correction process, demonstrating that the subtask correction module effectively resolves errors encountered during execution, ensures the successful completion of each subtask, and contributes to the overall success of the task.</p>
        </div>
        <div class="columns is-centered mt-4 mb-4">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/carouse4.png" alt="Subtask correction process visualization">
              <figcaption class="has-text-centered">Three cases of subtask correction process in the real-wrold environment. The first, second, and third rows in the figure correspond to the correction
processes of the three subtasks, where “(b) Trajectory0” denotes the initially planned trajectory, and “(e) Trajectory1” denotes the corrected trajectory, “(c) Obser-
vation0” and “(f) Observation0” correspond to the observation images obtained after executing “(b) Trajectory0” and “(e) Trajectory1”, respectively.</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section 4: Execution processes visualization -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered"> Execution processes visualizatio</h2>
        <div class="content has-text-justified">
          <p> Execution processes of three tasks in the real-world environment. These results clearly demonstrate that the RoboThink method effectively
addresses failures during execution, ensuring robust task completion and underscoring its reliability and advantages
in practical applications.</p>
        </div>
        <div class="columns is-centered mt-4 mb-4">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/carouse5.png" alt="Subtask correction process visualization">
              <figcaption class="has-text-centered">Execution processes of three tasks in the real-world environment. Where, red circles in the figure indicate the locations of subtask failures,
whereas green circles highlight successful corrections following failures.</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">If you find our work useful, please give us a free cite:</h2>
      <pre><code>
        
soon
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
